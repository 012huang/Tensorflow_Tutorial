{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n",
    "def summary(ndarr):\n",
    "    print(ndarr)\n",
    "    print(\"* shape: {}\".format(ndarr.shape))\n",
    "    print(\"* min: {}\".format(np.min(ndarr)))\n",
    "    print(\"* max: {}\".format(np.max(ndarr)))\n",
    "    print(\"* avg: {}\".format(np.mean(ndarr)))\n",
    "    print(\"* std: {}\".format(np.std(ndarr)))\n",
    "    print(\"* unique: {}\".format(np.unique(ndarr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "train_data = mnist.train\n",
    "valid_data = mnist.validation\n",
    "test_data = mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLogisticClassification(object):\n",
    "    def __init__(self,shape_picture,n_labels,learning_rate=0.5,dropout_ratio=0.5,alpha=0.0):\n",
    "        self.shape_picture = shape_picture\n",
    "        self.n_labels = n_labels\n",
    "        self.graph = tf.Graph()\n",
    "        self.build(learning_rate,dropout_ratio,alpha)\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        \n",
    "    def build(self,learning_rate,dropout_ratio,alpha):\n",
    "        with self.graph.as_default():\n",
    "            ### Input\n",
    "            self.train_pictures = tf.placeholder(tf.float32, shape=[None]+self.shape_picture)\n",
    "            self.train_labels   = tf.placeholder(tf.int32  , shape=(None,self.n_labels))\n",
    "            \n",
    "            ### Variable\n",
    "            self.weights = {\n",
    "                'conv1': tf.Variable(tf.truncated_normal(shape=(5,5,1,6),stddev=0.1)), \n",
    "                    # batch,28,28,3 => batch,24,24,6\n",
    "                'conv3': tf.Variable(tf.truncated_normal(shape=(5,5,6,16),stddev=0.1)),\n",
    "                    # batch,12,12,6 => batch,8,8,16\n",
    "                'fc6':   tf.Variable(tf.truncated_normal(shape=(4*4*16,84),stddev=0.1)),\n",
    "                    # batch,4x4x16 => batch,84\n",
    "                'fc7':   tf.Variable(tf.truncated_normal(shape=(84,self.n_labels),stddev=0.1)),\n",
    "                    # batch,84 => batch,10\n",
    "            }\n",
    "            self.biases  = {\n",
    "                'conv1': tf.Variable(tf.zeros( shape=(6) )),\n",
    "                'conv3': tf.Variable(tf.zeros( shape=(16) )),\n",
    "                'fc6':   tf.Variable(tf.zeros( shape=(84) )),\n",
    "                'fc7':   tf.Variable(tf.zeros( shape=(self.n_labels) )),\n",
    "            } \n",
    "\n",
    "            ### Optimalization\n",
    "            self.y_,self.original_loss = self.structure(pictures=self.train_pictures,\n",
    "                                                        labels=self.train_labels,\n",
    "                                                        dropout_ratio=dropout_ratio,\n",
    "                                                        train=True, )\n",
    "            \n",
    "            self.regularization = tf.reduce_mean(\n",
    "                                   [tf.nn.l2_loss(w)/tf.cast(tf.shape(w)[0]*tf.shape(w)[1],tf.float32)\n",
    "                                        for k,w in self.weights.items()])\n",
    "            self.loss = self.original_loss + alpha * self.regularization\n",
    "            \n",
    "            self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "            \n",
    "            ### Prediction\n",
    "            self.new_pictures = tf.placeholder(tf.float32, shape=[None]+self.shape_picture)\n",
    "            self.new_labels   = tf.placeholder(tf.int32  , shape=(None,self.n_labels))\n",
    "            self.new_y_,self.new_loss = self.structure(pictures=self.new_pictures,\n",
    "                                                       labels=self.new_labels,)\n",
    "            \n",
    "            ### Initialization\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    def structure(self,pictures,labels,dropout_ratio=None,train=False):\n",
    "        conv1 = self.getConv2DLayer(pictures,self.weights['conv1'],self.biases['conv1'],activation=tf.nn.relu)\n",
    "        pool2 = tf.nn.max_pool(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "        conv3 = self.getConv2DLayer(pool2,self.weights['conv3'],self.biases['conv3'],activation=tf.nn.relu)\n",
    "        pool4 = tf.nn.max_pool(conv3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "        fatten5 = self.getFlattenLayer(pool4)\n",
    "        if train: fatten5 = tf.nn.dropout(fatten5,keep_prob=1-dropout_ratio[0])\n",
    "        \n",
    "        fc6 = self.getDenseLayer(fatten5,self.weights['fc6'],self.biases['fc6'],activation=tf.nn.relu)\n",
    "        if train: fc6 = tf.nn.dropout(fc6,keep_prob=1-dropout_ratio[1])\n",
    "            \n",
    "        logits = self.getDenseLayer(fc6,self.weights['fc7'],self.biases['fc7'])\n",
    "        \n",
    "        y_ = tf.nn.softmax(logits)\n",
    "        loss = tf.reduce_mean(\n",
    "                 tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits))\n",
    "\n",
    "        return (y_,loss)\n",
    "    \n",
    "    def getDenseLayer(self,input_layer,weight,bias,activation=None):\n",
    "        x = tf.add(tf.matmul(input_layer,weight),bias)\n",
    "        if activation:\n",
    "            x = activation(x)\n",
    "        return x\n",
    "    \n",
    "    def getConv2DLayer(self,input_layer,weight,bias,strides=(1,1),padding='VALID',activation=None):\n",
    "        x = tf.add(\n",
    "              tf.nn.conv2d(input_layer,weight,[1,strides[0],strides[1],1], padding=padding),bias)\n",
    "        if activation:\n",
    "            x = activation(x)\n",
    "        return x\n",
    "\n",
    "    def getFlattenLayer(self,input_layer):\n",
    "        shape = input_layer.get_shape().as_list()\n",
    "        n = 1\n",
    "        for s in shape[1:]:\n",
    "            n *= s\n",
    "        x = tf.reshape(input_layer,[-1,n])\n",
    "        return x\n",
    "    \n",
    "    def fit(self,X,y,epochs=10,validation_data=None,test_data=None,batch_size=None):\n",
    "        X = self._check_array(X)\n",
    "        y = self._check_array(y)\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        random.seed(9000)\n",
    "        if not batch_size: batch_size=N\n",
    "        \n",
    "        self.sess.run(self.init_op)\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch %2d/%2d: \"%(epoch+1,epochs))\n",
    "            \n",
    "            # batch gradient descent\n",
    "            index = [i for i in range(N)]\n",
    "            random.shuffle(index)\n",
    "            while len(index)>0:\n",
    "                index_size = len(index)\n",
    "                batch_index = [index.pop() for _ in range(min(batch_size,index_size))]    \n",
    "            \n",
    "                feed_dict = {\n",
    "                    self.train_pictures: X[batch_index,:], \n",
    "                    self.train_labels: y[batch_index], \n",
    "                }\n",
    "                _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "                \n",
    "                print(\"[%d/%d] loss = %.4f     \" % ( N-len(index), N, loss ), end='\\r')\n",
    "\n",
    "            \n",
    "            # evaluate at the end of this epoch\n",
    "            y_ = self.predict(X)\n",
    "            train_loss = self.evaluate(X,y)\n",
    "            train_acc = self.accuracy(y_,y)\n",
    "            msg = \"[%d/%d] loss = %8.4f, acc = %3.2f%%\" % ( N, N, train_loss, train_acc*100 )\n",
    "            \n",
    "            if validation_data:\n",
    "                val_loss = self.evaluate(validation_data[0],validation_data[1])\n",
    "                val_acc = self.accuracy(self.predict(validation_data[0]),validation_data[1])\n",
    "                msg += \", val_loss = %8.4f, val_acc = %3.2f%%\" % ( val_loss, val_acc*100 )\n",
    "            \n",
    "            print(msg)\n",
    "            \n",
    "            \n",
    "        if test_data:\n",
    "            test_acc = self.accuracy(self.predict(test_data[0]),test_data[1])\n",
    "            print(\"test_acc = %3.2f%%\" % (test_acc*100))\n",
    "            \n",
    "    def accuracy(self, predictions, labels):\n",
    "        return (np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/predictions.shape[0])\n",
    "    \n",
    "    def predict(self,X):\n",
    "        X = self._check_array(X)\n",
    "        return self.sess.run(self.new_y_, feed_dict={self.new_pictures: X})\n",
    "    \n",
    "    def evaluate(self,X,y):\n",
    "        X = self._check_array(X)\n",
    "        y = self._check_array(y)\n",
    "        return self.sess.run(self.new_loss, feed_dict={self.new_pictures: X, self.new_labels: y})\n",
    "    \n",
    "    def _check_array(self,ndarray):\n",
    "        ndarray = np.array(ndarray)\n",
    "        if len(ndarray.shape)==1: ndarray = np.reshape(ndarray,(1,ndarray.shape[0]))\n",
    "        return ndarray\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10: \n",
      "[55000/55000] loss =   0.1285, acc = 96.14%, val_loss =   0.1263, val_acc = 96.30%\n",
      "Epoch  2/10: \n",
      "[55000/55000] loss =   0.0840, acc = 97.32%, val_loss =   0.0826, val_acc = 97.38%\n",
      "Epoch  3/10: \n",
      "[55000/55000] loss =   0.0822, acc = 97.49%, val_loss =   0.0801, val_acc = 97.84%\n",
      "Epoch  4/10: \n",
      "[55000/55000] loss =   0.0533, acc = 98.37%, val_loss =   0.0589, val_acc = 98.32%\n",
      "Epoch  5/10: \n",
      "[55000/55000] loss =   0.0470, acc = 98.55%, val_loss =   0.0537, val_acc = 98.36%\n",
      "Epoch  6/10: \n",
      "[55000/55000] loss =   0.0502, acc = 98.46%, val_loss =   0.0577, val_acc = 98.70%\n",
      "Epoch  7/10: \n",
      "[55000/55000] loss =   0.0465, acc = 98.57%, val_loss =   0.0587, val_acc = 98.36%\n",
      "Epoch  8/10: \n",
      "[55000/55000] loss =   0.0423, acc = 98.67%, val_loss =   0.0543, val_acc = 98.44%\n",
      "Epoch  9/10: \n",
      "[55000/55000] loss =   0.0441, acc = 98.65%, val_loss =   0.0546, val_acc = 98.60%\n",
      "Epoch 10/10: \n",
      "[55000/55000] loss =   0.0398, acc = 98.75%, val_loss =   0.0501, val_acc = 98.46%\n",
      "test_acc = 98.87%\n"
     ]
    }
   ],
   "source": [
    "model = CNNLogisticClassification(   shape_picture=[28,28,1],\n",
    "                                     n_labels=10,\n",
    "                                     learning_rate=0.06,\n",
    "                                     dropout_ratio=[0.2,0.6],\n",
    "                                     alpha=0.1,\n",
    "                                 )\n",
    "model.fit(X=np.reshape(train_data.images,[-1,28,28,1]),\n",
    "          y=train_data.labels,\n",
    "          epochs=10,\n",
    "          validation_data=(np.reshape(valid_data.images,[-1,28,28,1]),valid_data.labels),\n",
    "          test_data=(np.reshape(test_data.images,[-1,28,28,1]),test_data.labels),\n",
    "          batch_size = 32,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
